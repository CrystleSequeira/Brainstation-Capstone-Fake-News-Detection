{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# MLP: multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will run a number of different Neural Networks, starting with an out-of-the-box MLP Classifier, which I will then optimise. Next I will optimise a CNN and lastly an RNN. I expect the RNN to be the most successful as RNNs leverage the sequence of data which is certainly important for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Headline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TFIDF data\n",
    "X_train_head = pd.read_csv('data/X_train_head.csv')\n",
    "\n",
    "X_test_head = pd.read_csv('data/X_test_head.csv')\n",
    "\n",
    "y_train_head = pd.read_csv('data/y_train_head.csv')\n",
    "\n",
    "y_test_head = pd.read_csv('data/y_test_head.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an out-of-the-box MLPClassifier with one hidden layer\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(1), random_state=1)\n",
    "NN_model.fit(X_train_head, y_train_head);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6232445520581114"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.score(X_test_head, y_test_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the out-of-the-box MLPClassifier is similar in accuracy to the Logistic Regression with a 62.3% test accuracy. Currently the OOTB Random Forest is leading the way!\n",
    "\n",
    "This is not too surprising as I only have one hidden layer in my model which is of size, one unit, so it is behaving similar to a Logistic Regression model. It is unable to fit anything complex, so let's increase the number of hidden layers and the size of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an out-of-the-box Neural Network with 3 hidden layers of sizes (5, 10, 5)\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(5, 10, 5), random_state=1)\n",
    "NN_model.fit(X_train_head, y_train_head);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.636319612590799"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.score(X_test_head, y_test_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply adding two layers and increasing the size of the layers improved the accuracy by 1.3%. This already has it out in front of the other optimised models so far.\n",
    "\n",
    "Let's keep going! \n",
    "\n",
    "We will want to optimise over a number of paramters:\n",
    "\n",
    "1) hidden_layers: both the number of layers and their sizes \\\n",
    "2) solver: the algorithm for reducing the cost function in each epoch \\\n",
    "3) activation: the function determining the firing of a neuron between the layers \\\n",
    "4) alpha: regularisation parameter to reduce overfitting \\\n",
    "5) learning_rate_init: the size of the steps down the gradient of the solver function \\\n",
    "6) learning_rate: how the learning rate changes\n",
    "7) momentum: helps solvers get out of local minimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 63.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 86.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 478.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 591.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 642.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 708.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 760.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 830.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 978.8min\n",
      "[Parallel(n_jobs=-1)]: Done 8442 tasks      | elapsed: 1079.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9792 tasks      | elapsed: 1140.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11242 tasks      | elapsed: 1199.5min\n",
      "[Parallel(n_jobs=-1)]: Done 12792 tasks      | elapsed: 1264.3min\n",
      "[Parallel(n_jobs=-1)]: Done 12960 out of 12960 | elapsed: 1275.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(5, 10, 5), (50, 100, 50),\n",
    "                      (5, 10, 10, 10, 5), (50, 100, 100, 100, 50),\n",
    "                     (5, 10, 10, 10, 10, 10, 5), (10, 25, 25, 25, 25, 25, 10), ],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': [0.0001, 0.01, 1, 100, 1000],\n",
    "    'learning_rate_init': [0.00001, 0.001, 0.1, 10],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'momentum': [0.3, 0.6, 0.9]\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(MLPClassifier(), params, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "gridsearch_results = gridsearch.fit(X_train_head, y_train_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'alpha': 1,\n",
       " 'hidden_layer_sizes': (10, 25, 25, 25, 25, 25, 10),\n",
       " 'learning_rate': 'adaptive',\n",
       " 'learning_rate_init': 0.1,\n",
       " 'momentum': 0.6,\n",
       " 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = gridsearch_results.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6319923719578641"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_results.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of that, the best score I could achieve with my MLP Classifier is 63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the optimised MLPClassifier with optimal parameters\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=best_params[\"hidden_layer_sizes\"], \n",
    "                         activation=best_params[\"activation\"], \n",
    "                         alpha=best_params[\"alpha\"],\n",
    "                         learning_rate=best_params[\"learning_rate\"],\n",
    "                         learning_rate_init=best_params[\"learning_rate_init\"],\n",
    "                         momentum=best_params[\"momentum\"],\n",
    "                         solver=best_params[\"solver\"])\n",
    "NN_model.fit(X_train_head, y_train_head);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = NN_model.score(X_train_head, y_train_head)\n",
    "test_score = NN_model.score(X_test_head, y_test_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.6409542262048922\n",
      "Test score: 0.6372881355932203\n"
     ]
    }
   ],
   "source": [
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an MLPClassifier with more nodes int he hidden layers\n",
    "NN_model = MLPClassifier(hidden_layer_sizes=(100, 1000, 1000, 1000, 1000, 1000, 100), \n",
    "                         activation=best_params[\"activation\"], \n",
    "                         alpha=best_params[\"alpha\"],\n",
    "                         learning_rate=best_params[\"learning_rate\"],\n",
    "                         learning_rate_init=best_params[\"learning_rate_init\"],\n",
    "                         momentum=best_params[\"momentum\"],\n",
    "                         solver=best_params[\"solver\"])\n",
    "NN_model.fit(X_train_head, y_train_head);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = NN_model.score(X_train_head, y_train_head)\n",
    "test_score = NN_model.score(X_test_head, y_test_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.6313877452167596\n",
      "Test score: 0.6242130750605327\n"
     ]
    }
   ],
   "source": [
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
