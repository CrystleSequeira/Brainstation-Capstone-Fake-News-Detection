{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Vectorisation, CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally I would split up these notebooks into separate files but I did not know how to save the output of my Word2Vec Vectorisation for the inputs to my CNN, so I thought it best to run them all in the same notebook. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make full use of CNNs and RNNs I will need to develop a more computer readable input for my word data. I will also want to represent each document as a matrix so that I can apply filters in the case of CNNs and _______ for RNNs. \n",
    "\n",
    "You can create your own word embedding model by training it on your own dataset. However, this is can take time to build and run. And, unless you have a lot of data, it can produce poor results.\n",
    "\n",
    "There are number of different word embeddings available that have been prebuilt such as LexVec that was built from Wikipedia articles as well as Google's model that was made from Google News articles (https://machinelearningmastery.com/develop-word-embeddings-python-gensim/). Given this, I will leverage Google's prebuilt modle for my vectorisation as its domain is related to my dataset and it will be a more useful model than any I would be able to train on my small dataset as it used 100 billion words in its training...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages including Gensim package, download the Google model\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "data_filename = 'data/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(data_filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my model we can see that it has some really cool functionality. As each word is a vector in some dimensional space, we can look at the most similar words to a given word based on the closest word vectors to that word in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[('Calgary', 0.7786356210708618), ('Edmonton', 0.7367619276046753), ('Burnaby', 0.7355164885520935), ('Kelowna', 0.7330868244171143), ('Vancouver_BC', 0.7306336164474487), ('Chilliwack', 0.6999480128288269), ('British_Columbia', 0.6976038813591003), ('Nanaimo', 0.6958351135253906), ('Saskatoon', 0.6886626482009888), ('Kamloops', 0.6885457038879395)]\n"
     ]
    }
   ],
   "source": [
    "most_similar = model.most_similar(\"Vancouver\")\n",
    "print(model['Vancouver'].shape)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the closest words to Vancouver are other Canadian citiesor provinces, which makes a lot of intuitive sense. Also, note that each vector has 300 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my vectorisation model, I need to apply it to each document in my dataset. This will result in an output for each document that has d_i rows and n columns, where d_i equals the number words in my processed dataset and n is the number of dimensions of the word vectors (in this case 300).\n",
    "\n",
    "For each document I will end up with at t_i x n matrix, so will have over 8500 matrices in my training dataset and almost 2000 in my test dataset. I will need to create an array of these matrices for each of my training sets that will get passed into my CNNs. This is because for Keras, the input needs to be in the shape:\n",
    "\n",
    "(number of documents, number of rows, number of columns, number of channels) \n",
    "\n",
    "Where: \\\n",
    "number of documents = the number of vectorised matrices, one for each document \\\n",
    "number of rows = number of words in each document \\\n",
    "number of columns = number of dimensions in each vector \\\n",
    "number of channels = depth of each document matrix if not 2D (not relevant here)\n",
    "\n",
    "One issue here is that the number of rows, columns and channels need to be the same for all the images. For my datasets, however, the headlines have different numbers of words in them so the number of rows will not be the same for each document. The number of columns and channels will be the same as the dimension of each word vector is the same and I only have 1 channel for each document.\n",
    "\n",
    "To account for this, what I will do is vectorise each document and then see which document has the most rows. I will then add as many additional 0 vector rows as required for all other matrices so that they have the same number of rows.\n",
    "\n",
    "One issue with this method is that the model will need to learn that these 0 vector rows preovide no information about the headline, so this could impact my model's classification accuracy, but we shall see how it gets on.\n",
    "\n",
    "First I need to create my vectorised document matrices and store them in a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-processed data\n",
    "df_fake_news = pd.read_csv('data/preprocessed_fake_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Processed Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Processed Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>way Bob Corker skewer Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>image copyright Getty Images \\n Sunday morning...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>Linklater war veteran comedy speak modern Amer...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>LONDON Reuters flag Flying comedy drama Vietna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>Trump ’s Fight Corker Jeopardizes legislative ...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>feud break public view week Mr. Corker say Mr....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>Egypt Cheiron win tie Pemex mexican onshore oi...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>MEXICO CITY Reuters Egypt ’s Cheiron Holdings ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Jason Aldean open SNL Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>country singer Jason Aldean perform Las Vegas ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>State Department say find email Clinton specia...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>State Department tell Republican National Comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>p PBS stand plutocratic Pentagon</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>p PBS stand plutocratic Pentagon post Oct 27 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10320</th>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti trump Protesters tool Oligarchy      info...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>Anti trump Protesters tool Oligarchy reform...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321</th>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>Ethiopia Obama seek progress peace security Ea...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>ADDIS ABABA Ethiopia —President Obama convene ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush suddenly Attacking Trump matter</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush suddenly Attacking Trump matter \\n\\n ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10323 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Headline  \\\n",
       "0             Four ways Bob Corker skewered Donald Trump   \n",
       "1      Linklater's war veteran comedy speaks to moder...   \n",
       "2      Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3      Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4            Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "...                                                  ...   \n",
       "10318  State Department says it can't find emails fro...   \n",
       "10319  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "10320  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "10321  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "10322  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                      Processed Headline  \\\n",
       "0                     way Bob Corker skewer Donald Trump   \n",
       "1      Linklater war veteran comedy speak modern Amer...   \n",
       "2      Trump ’s Fight Corker Jeopardizes legislative ...   \n",
       "3      Egypt Cheiron win tie Pemex mexican onshore oi...   \n",
       "4                    Jason Aldean open SNL Vegas tribute   \n",
       "...                                                  ...   \n",
       "10318  State Department say find email Clinton specia...   \n",
       "10319                   p PBS stand plutocratic Pentagon   \n",
       "10320  Anti trump Protesters tool Oligarchy      info...   \n",
       "10321  Ethiopia Obama seek progress peace security Ea...   \n",
       "10322           Jeb Bush suddenly Attacking Trump matter   \n",
       "\n",
       "                                                    Body  \\\n",
       "0      Image copyright Getty Images\\nOn Sunday mornin...   \n",
       "1      LONDON (Reuters) - “Last Flag Flying”, a comed...   \n",
       "2      The feud broke into public view last week when...   \n",
       "3      MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...   \n",
       "4      Country singer Jason Aldean, who was performin...   \n",
       "...                                                  ...   \n",
       "10318  The State Department told the Republican Natio...   \n",
       "10319  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "10320   Anti-Trump Protesters Are Tools of the Oligar...   \n",
       "10321  ADDIS ABABA, Ethiopia —President Obama convene...   \n",
       "10322  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                          Processed Body  Label  \n",
       "0      image copyright Getty Images \\n Sunday morning...      1  \n",
       "1      LONDON Reuters flag Flying comedy drama Vietna...      1  \n",
       "2      feud break public view week Mr. Corker say Mr....      1  \n",
       "3      MEXICO CITY Reuters Egypt ’s Cheiron Holdings ...      1  \n",
       "4      country singer Jason Aldean perform Las Vegas ...      1  \n",
       "...                                                  ...    ...  \n",
       "10318  State Department tell Republican National Comm...      1  \n",
       "10319  p PBS stand plutocratic Pentagon post Oct 27 2...      0  \n",
       "10320    Anti trump Protesters tool Oligarchy reform...      0  \n",
       "10321  ADDIS ABABA Ethiopia —President Obama convene ...      1  \n",
       "10322  Jeb Bush suddenly Attacking Trump matter \\n\\n ...      1  \n",
       "\n",
       "[10323 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to drop the Body columns as I am no longer using them for my models. I also don't require the original Headline column as I will only be vectorising the Processed Headline column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop body columns\n",
    "df_fake_news.drop(['Body', 'Processed Body', 'Headline'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processed Headline    6\n",
       "Label                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_news.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discovered before we have some NaN entries in the headlines so I will remove these before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake_news.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processed Headline    0\n",
       "Label                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake_news.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Note: \n",
    "In preparing my data input for the CNN function I found out that while padding the matrices to be the same size one of the headlines had 65 words. This is an issue as the average headline in my dataset has 7 words. So that means a lot of the matrices will have almost 10x more padded zero vector rows than non-zero vector rows. So I decided I will remove any headlines longer than 13 words. This is becuase less than 3% of all headlines in my training set had more than 13 words in the headline.  (See below)\n",
    "\n",
    "By removing these longer headlines I reduce the amount of required padding on all my matrices which will save space and computation time. It will also make it easier for the model to pick out which vectors are relevant as there won't be a bunch of zero vectors in each matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percent of headlines for threshold 30: 0.02907822041291073\n",
      " Percent of headlines for threshold 25: 0.04846370068818455\n",
      " Percent of headlines for threshold 20: 0.12600562178927982\n",
      " Percent of headlines for threshold 15: 0.9789667539013279\n",
      " Percent of headlines for threshold 13: 2.5201124357855966\n"
     ]
    }
   ],
   "source": [
    "# Test to see what headline length cutoff threshold would be best to remove long headlines\n",
    "\n",
    "thresh_30 = 0\n",
    "thresh_25 = 0\n",
    "thresh_20 = 0\n",
    "thresh_15 = 0\n",
    "thresh_13 = 0\n",
    "thresh_10 = 0\n",
    "\n",
    "for index, row in df_fake_news.iterrows():\n",
    "    num_headlines = df_fake_news.shape[0]\n",
    "    headline = row[0]\n",
    "    headline_words = headline.split(' ')\n",
    "    num_words = len(headline_words)\n",
    "    \n",
    "    if num_words > 30:\n",
    "        thresh_30 += 1\n",
    "        continue\n",
    "        \n",
    "    elif num_words > 25:\n",
    "        thresh_25 += 1\n",
    "        continue\n",
    "        \n",
    "    elif num_words > 20:\n",
    "        thresh_20 += 1\n",
    "        continue\n",
    "        \n",
    "    elif num_words > 15:\n",
    "        thresh_15 += 1\n",
    "        continue\n",
    "        \n",
    "    elif num_words > 13:\n",
    "        thresh_13 += 1\n",
    "        continue\n",
    "        \n",
    "    elif num_words > 10:\n",
    "        thresh_10 += 1\n",
    "        continue\n",
    "\n",
    "print(f' Percent of headlines for threshold 30: {100*thresh_30/num_headlines}')\n",
    "print(f' Percent of headlines for threshold 25: {100*(thresh_30 + thresh_25)/num_headlines}')\n",
    "print(f' Percent of headlines for threshold 20: {100*(thresh_30 + thresh_25 + thresh_20)/num_headlines}')\n",
    "print(f' Percent of headlines for threshold 15: {100*(thresh_30 + thresh_25 + thresh_20 + thresh_15)/num_headlines}')\n",
    "print(f' Percent of headlines for threshold 13: {100*(thresh_30 + thresh_25 + thresh_20 + thresh_15 + thresh_13)/num_headlines}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows from the original dataset that have headlines with more than 13 words. Save this dataset.\n",
    "\n",
    "for index, row in df_fake_news.iterrows():\n",
    "    \n",
    "    words = row[0].split(' ')\n",
    "    num_words = len(words)\n",
    "    \n",
    "    if num_words > 13:\n",
    "        df_fake_news.drop(labels=index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a dataset that is just the pre-processed Headlines with the associated classification labels and each Headline has 13 words or less, I will now split this into my train and test sets and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_fake_news.iloc[:, 0]\n",
    "y = df_fake_news.iloc[:, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have my training and test data sets, it is time to convert each document into a vectorised matrix using the Google embedding. To do this I will create a function that generates an array of vectorised matrices given a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDocArray(headline_words):\n",
    "    \n",
    "    doc_array = []\n",
    "    \n",
    "    # Loop over each word in the headline\n",
    "    for word in headline_words:\n",
    "\n",
    "        # Vectorise each word and create a Data Frame for the word. If word is not in the model, \n",
    "        # print the word and move on to the next word in the headline\n",
    "        try:\n",
    "            word_vec = model[word]\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # Join the new word onto the document matrix along the columns\n",
    "        doc_array.append(word_vec)   \n",
    "        \n",
    "    return doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padDocument(doc_array, max_length):\n",
    "    \n",
    "    # Create a zero_vector to pad matrices with shape (1,300)\n",
    "    zero_vector = pd.DataFrame(np.zeros((1,300)))\n",
    "    \n",
    "    # Get the current number of rows in the document array\n",
    "    num_rows = len(doc_array)\n",
    "    \n",
    "    # Find how many padded rows are needed\n",
    "    missing_rows = max_length - num_rows\n",
    "    \n",
    "    for i in np.arange(missing_rows):\n",
    "        doc_array.append(zero_vector)\n",
    "        \n",
    "    return doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentVectoriser(df, max_length):\n",
    "\n",
    "    # Output array of vectorised matrices\n",
    "    output_array = []\n",
    "\n",
    "    # Loop over every entry in my training set\n",
    "    for i in np.arange(df.shape[0]):\n",
    "        \n",
    "        # Extract each headline\n",
    "        headline = df.iloc[i]\n",
    "\n",
    "        # Convert the string into words\n",
    "        headline_words = headline.str.split(' ')\n",
    "\n",
    "        #Create a dataframe for the document\n",
    "        doc_array = []\n",
    "\n",
    "        # Create the document array\n",
    "        doc_array = buildDocArray(headline_words)\n",
    "         \n",
    "        # Pad the document array as needed\n",
    "        doc_array = padDocument(doc_array, max_length)\n",
    "\n",
    "        # Add the new document array to the output array\n",
    "        output_array.append(doc_array)\n",
    "\n",
    "    return np.array(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = documentVectoriser(X_train, 13)\n",
    "test_inputs = documentVectoriser(X_test, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8045, 13, 300)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I have vectorised all my headlines. In the process I realised some of the words used in the headlines were not recognised my Google's model so they were dropped from the vectorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific neural network models & layer types\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally at the exciting part of the modelling, CNNs! To start I will warm up with a simple CNN taken from our in-class lecture (modified for size) to give us a baseline accuracy.\n",
    "\n",
    "First I need to make sure my training and test data are in the correct shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (8045, 13, 300)\n",
      "Test input shape: (2012, 13, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training input shape: {train_inputs.shape}')\n",
    "print(f'Test input shape: {test_inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN needs input in the form of a 4D tensor where the input is of shape: \\\n",
    "\n",
    "(n_documents, num_words, n_dim_word_vector, num_channels) \\\n",
    "\n",
    "Currently we have our inputs in the following shape:\n",
    "\n",
    "(n_documents, num_words, n_dim_word_vector) \\\n",
    "\n",
    "So we just need to reshape and add in the num_channels, which in this case is just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input image dimensions\n",
    "num_words, n_dim, num_channels = 13, 300, 1\n",
    "\n",
    "# Reshape for Keras model types\n",
    "X_train_CNN = train_inputs.reshape(train_inputs.shape[0], num_words, n_dim, 1)\n",
    "X_test_CNN = test_inputs.reshape(test_inputs.shape[0], num_words, n_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 11, 298, 512)      5120      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 296, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 4, 148, 1024)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 148, 1024)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 606208)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                7274508   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 26        \n",
      "=================================================================\n",
      "Total params: 11,999,270\n",
      "Trainable params: 11,999,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "CNN_model = Sequential()\n",
    "\n",
    "CNN_model.add(Conv2D(512, kernel_size=(3, 3), activation = 'relu', input_shape = (num_words, n_dim, num_channels)))\n",
    "CNN_model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "CNN_model.add(Dropout(0.25))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(12, activation='relu'))\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the desired loss function, optimizer, and metric to optimize\n",
    "CNN_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So running this model initially gave me a runtime of 5 days. To improve this I will take a 10% subsample of my training and test data so that it hopefully only runs in 12 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sub-sample of my training and test data set\n",
    "\n",
    "X_sub_train_indices = np.random.random_integers(0, X_train_CNN.shape[0], 850)\n",
    "X_sub_test_indices = np.random.random_integers(0, X_test_CNN.shape[0], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_train = X_train_CNN[X_sub_train_indices]\n",
    "X_sub_test = X_test_CNN[X_sub_test_indices]\n",
    "y_sub_train = y_train.iloc[X_sub_train_indices]\n",
    "y_sub_test = y_test.iloc[X_sub_test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 850 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "850/850 [==============================] - 19264s 23s/step - loss: 0.5517 - acc: 0.7176 - val_loss: 0.5565 - val_acc: 0.7400\n",
      "850/850 [==============================] - 11787s 14s/step - loss: 0.5141 - acc: 0.7718 - val_loss: 0.5611 - val_acc: 0.7500\n",
      "Epoch 3/10\n",
      "850/850 [==============================] - 3059s 4s/step - loss: 0.5065 - acc: 0.7376 - val_loss: 0.5664 - val_acc: 0.7300\n",
      "Epoch 4/10\n",
      "850/850 [==============================] - 3041s 4s/step - loss: 0.4808 - acc: 0.7506 - val_loss: 0.5677 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "850/850 [==============================] - 2712s 3s/step - loss: 0.4423 - acc: 0.7800 - val_loss: 0.5674 - val_acc: 0.7350\n",
      "Epoch 6/10\n",
      "850/850 [==============================] - 2721s 3s/step - loss: 0.3907 - acc: 0.7976 - val_loss: 0.5398 - val_acc: 0.7400\n",
      "Epoch 7/10\n",
      "850/850 [==============================] - 4052s 5s/step - loss: 0.3963 - acc: 0.7953 - val_loss: 0.6011 - val_acc: 0.7450\n",
      "Epoch 8/10\n",
      "850/850 [==============================] - 6603s 8s/step - loss: 0.3559 - acc: 0.8000 - val_loss: 0.6283 - val_acc: 0.7400\n",
      "Epoch 9/10\n",
      "850/850 [==============================] - 5779s 7s/step - loss: 0.3512 - acc: 0.8412 - val_loss: 0.6119 - val_acc: 0.7250\n",
      "Epoch 10/10\n",
      "850/850 [==============================] - 3912s 5s/step - loss: 0.3425 - acc: 0.8282 - val_loss: 0.7058 - val_acc: 0.7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a1d41cb38>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the training data, defining desired batch_size & number of epochs,\n",
    "# running validation on the test data after each batch\n",
    "# THIS WILL TAKE A LONG TIME TO RUN!!!\n",
    "CNN_model.fit(X_sub_train, y_sub_train,\n",
    "              batch_size = 128,\n",
    "              epochs = 10,\n",
    "              verbose = 1,\n",
    "              validation_data = (X_sub_test, y_sub_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 201s 1s/step\n",
      "Test loss: 0.7057784414291381\n",
      "Test accuracy: 0.745\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = CNN_model.evaluate(X_sub_test, y_sub_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running for 24 hours we managed to achieve a test accuracy of 74.5%. This is a significant improvement from our previous models and so I will save this CNN to file.\n",
    "\n",
    "Ref: https://machinelearningmastery.com/save-load-keras-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = CNN_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "CNN_model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 192s 958ms/step\n",
      "acc: 74.50%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])\n",
    "score = loaded_model.evaluate(X_sub_test, y_sub_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model I will be using is a Recurrernt Neural Network (RNN). I have high expectations that this will be the best model for my dataset as RNNs leverage the connectedness of a dataset. In my case, the sequence of words in my headlines plays an important role and an RNN is able to use the sequence as input for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data input for an RNN is almost the the same as for the CNN, but we no longer require the num_channels. So I just need to reshape the data. For this I will use the same sub-sample as for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the sub-sample training and test data\n",
    "\n",
    "X_train_RNN_sub_sample = np.reshape(X_sub_train, (X_sub_train.shape[0], X_sub_train.shape[1],X_sub_train.shape[2]))\n",
    "X_test_RNN_sub_sample = np.reshape(X_sub_test, (X_sub_test.shape[0], X_sub_test.shape[1], X_sub_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 13, 300)\n",
      "(200, 13, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_RNN_sub_sample.shape)\n",
    "print(X_test_RNN_sub_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "850 = # headlines \\\n",
    "13 = # words in each headline (including padded zero vectors) \\\n",
    "300 = dimension of the embedded word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our RNN\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "RNN_model = Sequential()\n",
    "\n",
    "RNN_model.add(LSTM(2048, activation='relu', return_sequences=True))\n",
    "RNN_model.add(Dropout(0.2))\n",
    "RNN_model.add(BatchNormalization())\n",
    "\n",
    "RNN_model.add(LSTM(618, activation='relu', return_sequences=True))\n",
    "RNN_model.add(Dropout(0.2))\n",
    "RNN_model.add(BatchNormalization())\n",
    "\n",
    "RNN_model.add(LSTM(1024, activation='relu'))\n",
    "RNN_model.add(Dropout(0.2))\n",
    "RNN_model.add(BatchNormalization())\n",
    "\n",
    "RNN_model.add(Dense(32, activation='relu'))\n",
    "RNN_model.add(Dropout(0.1))\n",
    "\n",
    "RNN_model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "RNN_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an image of its architecture to file\n",
    "plot_model(RNN_model, to_file='data/RNN_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 850 samples, validate on 200 samples\n",
      "Epoch 1/40\n",
      "850/850 [==============================] - 272s 321ms/step - loss: 1.5281 - acc: 0.5765 - val_loss: 0.6909 - val_acc: 0.5250\n",
      "Epoch 2/40\n",
      "850/850 [==============================] - 61s 71ms/step - loss: 1.0073 - acc: 0.6859 - val_loss: 0.6908 - val_acc: 0.5250\n",
      "Epoch 3/40\n",
      "850/850 [==============================] - 58s 68ms/step - loss: 0.4922 - acc: 0.7882 - val_loss: 1.2555 - val_acc: 0.5250\n",
      "Epoch 4/40\n",
      "850/850 [==============================] - 60s 71ms/step - loss: 0.4832 - acc: 0.7776 - val_loss: 7.3505 - val_acc: 0.5250\n",
      "Epoch 5/40\n",
      "850/850 [==============================] - 57s 67ms/step - loss: 0.3903 - acc: 0.8141 - val_loss: 7.6561 - val_acc: 0.5250\n",
      "Epoch 6/40\n",
      "850/850 [==============================] - 56s 66ms/step - loss: 0.3363 - acc: 0.8376 - val_loss: 7.6561 - val_acc: 0.5250\n",
      "Epoch 7/40\n",
      "850/850 [==============================] - 62s 73ms/step - loss: 0.2698 - acc: 0.8847 - val_loss: 7.6561 - val_acc: 0.5250\n",
      "Epoch 8/40\n",
      "850/850 [==============================] - 54s 63ms/step - loss: 0.1673 - acc: 0.9294 - val_loss: 7.6561 - val_acc: 0.5250\n",
      "Epoch 9/40\n",
      "850/850 [==============================] - 55s 64ms/step - loss: 0.0978 - acc: 0.9682 - val_loss: 7.6561 - val_acc: 0.5250\n",
      "Epoch 10/40\n",
      "850/850 [==============================] - 52s 61ms/step - loss: 0.0574 - acc: 0.9800 - val_loss: 7.6435 - val_acc: 0.5250\n",
      "Epoch 11/40\n",
      "850/850 [==============================] - 50s 59ms/step - loss: 0.0484 - acc: 0.9859 - val_loss: 7.6203 - val_acc: 0.5250\n",
      "Epoch 12/40\n",
      "850/850 [==============================] - 50s 59ms/step - loss: 0.0542 - acc: 0.9824 - val_loss: 7.3334 - val_acc: 0.5250\n",
      "Epoch 13/40\n",
      "850/850 [==============================] - 55s 64ms/step - loss: 0.0328 - acc: 0.9882 - val_loss: 7.4337 - val_acc: 0.5250\n",
      "Epoch 14/40\n",
      "850/850 [==============================] - 52s 61ms/step - loss: 0.0391 - acc: 0.9894 - val_loss: 6.4999 - val_acc: 0.5250\n",
      "Epoch 15/40\n",
      "850/850 [==============================] - 54s 63ms/step - loss: 0.0341 - acc: 0.9847 - val_loss: 6.2940 - val_acc: 0.5250\n",
      "Epoch 16/40\n",
      "850/850 [==============================] - 64s 75ms/step - loss: 0.0241 - acc: 0.9941 - val_loss: 3.9228 - val_acc: 0.5250\n",
      "Epoch 17/40\n",
      "850/850 [==============================] - 52s 61ms/step - loss: 0.0331 - acc: 0.9871 - val_loss: 6.7695 - val_acc: 0.5250\n",
      "Epoch 18/40\n",
      "850/850 [==============================] - 55s 65ms/step - loss: 0.0212 - acc: 0.9906 - val_loss: 4.2972 - val_acc: 0.5250\n",
      "Epoch 19/40\n",
      "850/850 [==============================] - 54s 64ms/step - loss: 0.0105 - acc: 0.9953 - val_loss: 3.9910 - val_acc: 0.5250\n",
      "Epoch 20/40\n",
      "850/850 [==============================] - 54s 63ms/step - loss: 0.0193 - acc: 0.9918 - val_loss: 4.3821 - val_acc: 0.5250\n",
      "Epoch 21/40\n",
      "850/850 [==============================] - 58s 68ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 4.7612 - val_acc: 0.5250\n",
      "Epoch 22/40\n",
      "850/850 [==============================] - 60s 71ms/step - loss: 0.0119 - acc: 0.9965 - val_loss: 3.1371 - val_acc: 0.5250\n",
      "Epoch 23/40\n",
      "850/850 [==============================] - 57s 67ms/step - loss: 0.0128 - acc: 0.9941 - val_loss: 4.6981 - val_acc: 0.5250\n",
      "Epoch 24/40\n",
      "850/850 [==============================] - 54s 63ms/step - loss: 0.0070 - acc: 0.9965 - val_loss: 4.1806 - val_acc: 0.5250\n",
      "Epoch 25/40\n",
      "850/850 [==============================] - 62s 73ms/step - loss: 0.0222 - acc: 0.9918 - val_loss: 6.4411 - val_acc: 0.5250\n",
      "Epoch 26/40\n",
      "850/850 [==============================] - 53s 63ms/step - loss: 0.0037 - acc: 0.9988 - val_loss: 1.4338 - val_acc: 0.5250\n",
      "Epoch 27/40\n",
      "850/850 [==============================] - 67s 79ms/step - loss: 0.0244 - acc: 0.9929 - val_loss: 2.3558 - val_acc: 0.5250\n",
      "Epoch 28/40\n",
      "850/850 [==============================] - 81s 95ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 4.6445 - val_acc: 0.5250\n",
      "Epoch 29/40\n",
      "850/850 [==============================] - 64s 75ms/step - loss: 0.0407 - acc: 0.9941 - val_loss: 1.8394 - val_acc: 0.5250\n",
      "Epoch 30/40\n",
      "850/850 [==============================] - 55s 64ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.9689 - val_acc: 0.5250\n",
      "Epoch 31/40\n",
      "850/850 [==============================] - 53s 62ms/step - loss: 0.0250 - acc: 0.9906 - val_loss: 2.1491 - val_acc: 0.5250\n",
      "Epoch 32/40\n",
      "850/850 [==============================] - 54s 64ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 1.6822 - val_acc: 0.5250\n",
      "Epoch 33/40\n",
      "850/850 [==============================] - 60s 71ms/step - loss: 0.0226 - acc: 0.9906 - val_loss: 1.3198 - val_acc: 0.5600\n",
      "Epoch 34/40\n",
      "850/850 [==============================] - 57s 67ms/step - loss: 0.0279 - acc: 0.9871 - val_loss: 3.2882 - val_acc: 0.5250\n",
      "Epoch 35/40\n",
      "850/850 [==============================] - 52s 61ms/step - loss: 0.0077 - acc: 0.9965 - val_loss: 2.0294 - val_acc: 0.5250\n",
      "Epoch 36/40\n",
      "850/850 [==============================] - 56s 66ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 2.2865 - val_acc: 0.5250\n",
      "Epoch 37/40\n",
      "850/850 [==============================] - 54s 63ms/step - loss: 0.0129 - acc: 0.9941 - val_loss: 1.3748 - val_acc: 0.5700\n",
      "Epoch 38/40\n",
      "850/850 [==============================] - 79s 93ms/step - loss: 0.0168 - acc: 0.9941 - val_loss: 1.4493 - val_acc: 0.5400\n",
      "Epoch 39/40\n",
      "850/850 [==============================] - 57s 67ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 1.1984 - val_acc: 0.5750\n",
      "Epoch 40/40\n",
      "850/850 [==============================] - 56s 65ms/step - loss: 0.0084 - acc: 0.9965 - val_loss: 1.1123 - val_acc: 0.5800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb428c6d8>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_RNN_test = np.array(y_sub_test)\n",
    "y_RNN_train = np.array(y_sub_train)\n",
    "\n",
    "EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "BATCH_SIZE = 480   # at each epoch, it will split the data into units of 48 samples, and train on those\n",
    "\n",
    "RNN_model.fit(X_train_RNN_sub_sample, y_RNN_train,\n",
    "               batch_size=BATCH_SIZE,\n",
    "               epochs=EPOCHS,\n",
    "              verbose=1,\n",
    "               validation_data = (X_test_RNN_sub_sample, y_RNN_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the RNN with a 10% subsample managed to achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 6s 30ms/step\n",
      "Test loss: 1.1123304891586303\n",
      "Test accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = RNN_model.evaluate(X_test_RNN_sub_sample, y_RNN_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "RNN_model_json = RNN_model.to_json()\n",
    "with open(\"RNN_model.json\", \"w\") as json_file:\n",
    "    json_file.write(RNN_model_json)\n",
    "# serialize weights to HDF5\n",
    "RNN_model.save_weights(\"RNN_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "RNN_json_file = open('RNN_model.json', 'r')\n",
    "RNN_loaded_model_json = RNN_json_file.read()\n",
    "RNN_json_file.close()\n",
    "RNN_loaded_model = model_from_json(RNN_loaded_model_json)\n",
    "# load weights into new model\n",
    "RNN_loaded_model.load_weights(\"RNN_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "RNN_loaded_model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])\n",
    "score = RNN_loaded_model.evaluate(X_sub_test, y_RNN_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (RNN_loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "RNN_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8045 samples, validate on 2012 samples\n",
      "Epoch 1/40\n",
      "8045/8045 [==============================] - 597s 74ms/step - loss: 0.6194 - acc: 0.7595 - val_loss: 0.7575 - val_acc: 0.4891\n",
      "Epoch 2/40\n",
      "8045/8045 [==============================] - 470s 58ms/step - loss: 0.4075 - acc: 0.8183 - val_loss: 0.7620 - val_acc: 0.4891\n",
      "Epoch 3/40\n",
      "8045/8045 [==============================] - 456s 57ms/step - loss: 0.3318 - acc: 0.8564 - val_loss: 0.7380 - val_acc: 0.4896\n",
      "Epoch 4/40\n",
      "8045/8045 [==============================] - 2603s 324ms/step - loss: 0.2301 - acc: 0.9057 - val_loss: 0.6884 - val_acc: 0.4896\n",
      "Epoch 5/40\n",
      "8045/8045 [==============================] - 1920s 239ms/step - loss: 0.1672 - acc: 0.9344 - val_loss: 0.7533 - val_acc: 0.4920\n",
      "Epoch 6/40\n",
      "8045/8045 [==============================] - 454s 56ms/step - loss: 0.1323 - acc: 0.9485 - val_loss: 1.9529 - val_acc: 0.5383\n",
      "Epoch 7/40\n",
      "8045/8045 [==============================] - 451s 56ms/step - loss: 0.0941 - acc: 0.9649 - val_loss: 0.5853 - val_acc: 0.7381\n",
      "Epoch 8/40\n",
      "8045/8045 [==============================] - 485s 60ms/step - loss: 0.0790 - acc: 0.9719 - val_loss: 0.7360 - val_acc: 0.4970\n",
      "Epoch 9/40\n",
      "8045/8045 [==============================] - 453s 56ms/step - loss: 0.0648 - acc: 0.9780 - val_loss: 0.7360 - val_acc: 0.4896\n",
      "Epoch 10/40\n",
      "8045/8045 [==============================] - 492s 61ms/step - loss: 0.0502 - acc: 0.9806 - val_loss: 0.6138 - val_acc: 0.6133\n",
      "Epoch 11/40\n",
      "8045/8045 [==============================] - 507s 63ms/step - loss: 0.0520 - acc: 0.9811 - val_loss: 0.5911 - val_acc: 0.8171\n",
      "Epoch 12/40\n",
      "8045/8045 [==============================] - 496s 62ms/step - loss: 0.0333 - acc: 0.9901 - val_loss: 0.5893 - val_acc: 0.7137\n",
      "Epoch 13/40\n",
      "8045/8045 [==============================] - 502s 62ms/step - loss: 0.0331 - acc: 0.9889 - val_loss: 0.5652 - val_acc: 0.7351\n",
      "Epoch 14/40\n",
      "8045/8045 [==============================] - 501s 62ms/step - loss: 0.0320 - acc: 0.9892 - val_loss: 0.5976 - val_acc: 0.6337\n",
      "Epoch 15/40\n",
      "8045/8045 [==============================] - 486s 60ms/step - loss: 0.0297 - acc: 0.9896 - val_loss: 0.7617 - val_acc: 0.5472\n",
      "Epoch 16/40\n",
      "8045/8045 [==============================] - 488s 61ms/step - loss: 0.0275 - acc: 0.9902 - val_loss: 0.9801 - val_acc: 0.7778\n",
      "Epoch 17/40\n",
      "8045/8045 [==============================] - 489s 61ms/step - loss: 0.0246 - acc: 0.9925 - val_loss: 0.5284 - val_acc: 0.7142\n",
      "Epoch 18/40\n",
      "8045/8045 [==============================] - 476s 59ms/step - loss: 0.0338 - acc: 0.9894 - val_loss: 0.4737 - val_acc: 0.7942\n",
      "Epoch 19/40\n",
      "8045/8045 [==============================] - 469s 58ms/step - loss: 0.0229 - acc: 0.9932 - val_loss: 0.7596 - val_acc: 0.6039\n",
      "Epoch 20/40\n",
      "8045/8045 [==============================] - 474s 59ms/step - loss: 0.0280 - acc: 0.9909 - val_loss: 0.4267 - val_acc: 0.8176\n",
      "Epoch 21/40\n",
      "8045/8045 [==============================] - 5805s 722ms/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.4547 - val_acc: 0.7868\n",
      "Epoch 22/40\n",
      "8045/8045 [==============================] - 485s 60ms/step - loss: 0.0277 - acc: 0.9899 - val_loss: 0.4556 - val_acc: 0.7883\n",
      "Epoch 23/40\n",
      "8045/8045 [==============================] - 483s 60ms/step - loss: 0.0224 - acc: 0.9930 - val_loss: 0.4656 - val_acc: 0.8206\n",
      "Epoch 24/40\n",
      "8045/8045 [==============================] - 513s 64ms/step - loss: 0.0168 - acc: 0.9952 - val_loss: 0.5047 - val_acc: 0.7798\n",
      "Epoch 25/40\n",
      "8045/8045 [==============================] - 520s 65ms/step - loss: 0.0247 - acc: 0.9919 - val_loss: 1.9574 - val_acc: 0.7475\n",
      "Epoch 26/40\n",
      "8045/8045 [==============================] - 581s 72ms/step - loss: 0.0138 - acc: 0.9963 - val_loss: 0.4556 - val_acc: 0.8181\n",
      "Epoch 27/40\n",
      "8045/8045 [==============================] - 512s 64ms/step - loss: 0.0146 - acc: 0.9960 - val_loss: 0.4981 - val_acc: 0.7942\n",
      "Epoch 28/40\n",
      "8045/8045 [==============================] - 579s 72ms/step - loss: 0.0098 - acc: 0.9956 - val_loss: 0.4528 - val_acc: 0.8250\n",
      "Epoch 29/40\n",
      "8045/8045 [==============================] - 552s 69ms/step - loss: 0.0168 - acc: 0.9953 - val_loss: 0.4405 - val_acc: 0.8151\n",
      "Epoch 30/40\n",
      "8045/8045 [==============================] - 577s 72ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.4597 - val_acc: 0.8216\n",
      "Epoch 31/40\n",
      "8045/8045 [==============================] - 555s 69ms/step - loss: 0.0068 - acc: 0.9983 - val_loss: 0.4903 - val_acc: 0.8186\n",
      "Epoch 32/40\n",
      "8045/8045 [==============================] - 579s 72ms/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.4665 - val_acc: 0.8295\n",
      "Epoch 33/40\n",
      "8045/8045 [==============================] - 635s 79ms/step - loss: 0.0093 - acc: 0.9973 - val_loss: 0.4831 - val_acc: 0.8300\n",
      "Epoch 34/40\n",
      "8045/8045 [==============================] - 1220s 152ms/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.4836 - val_acc: 0.8206\n",
      "Epoch 35/40\n",
      "8045/8045 [==============================] - 1037s 129ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.5578 - val_acc: 0.8270\n",
      "Epoch 36/40\n",
      "8045/8045 [==============================] - 1025s 127ms/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.5736 - val_acc: 0.8246\n",
      "Epoch 37/40\n",
      "8045/8045 [==============================] - 3210s 399ms/step - loss: 0.0109 - acc: 0.9964 - val_loss: 0.6120 - val_acc: 0.8042\n",
      "Epoch 38/40\n",
      "8045/8045 [==============================] - 1199s 149ms/step - loss: 0.0177 - acc: 0.9945 - val_loss: 0.5971 - val_acc: 0.7937\n",
      "Epoch 39/40\n",
      "8045/8045 [==============================] - 2794s 347ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.6263 - val_acc: 0.8236\n",
      "Epoch 40/40\n",
      "8045/8045 [==============================] - 2058s 256ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.6379 - val_acc: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b46db3080>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 40       # NNs operate in epochs, meaning this is how many times the neural network will go through \n",
    "                      # the entire data\n",
    "BATCH_SIZE = 480   # at each epoch, it will split the data into units of 48 samples, and train on those\n",
    "\n",
    "RNN_model.fit(train_inputs, y_train,\n",
    "               batch_size=BATCH_SIZE,\n",
    "               epochs=EPOCHS,\n",
    "              verbose=1,\n",
    "               validation_data = (test_inputs, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 13s 65ms/step\n",
      "Test loss: 0.7389770400524139\n",
      "Test accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = RNN_model.evaluate(X_test_RNN_sub_sample, y_RNN_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
