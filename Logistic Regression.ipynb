{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF-IDF data\n",
    "X_train = pd.read_csv('data/tfidf_X_train.csv')\n",
    "\n",
    "X_test= pd.read_csv('data/tfidf_X_test.csv')\n",
    "\n",
    "y_train = pd.read_csv('data/tfidf_y_train.csv')\n",
    "\n",
    "y_test = pd.read_csv('data/tfidf_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>h_10</th>\n",
       "      <th>h_2016</th>\n",
       "      <th>h_2017</th>\n",
       "      <th>h_america</th>\n",
       "      <th>h_attack</th>\n",
       "      <th>h_big</th>\n",
       "      <th>h_campaign</th>\n",
       "      <th>h_clinton</th>\n",
       "      <th>h_deal</th>\n",
       "      <th>...</th>\n",
       "      <th>b_york city</th>\n",
       "      <th>b_york times</th>\n",
       "      <th>b_york times newsletter</th>\n",
       "      <th>b_york times product</th>\n",
       "      <th>b_young</th>\n",
       "      <th>b_young people</th>\n",
       "      <th>b_youth</th>\n",
       "      <th>b_youtube</th>\n",
       "      <th>b_zero</th>\n",
       "      <th>b_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8253</th>\n",
       "      <td>8253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>8254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8255</th>\n",
       "      <td>8255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>8256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8257</th>\n",
       "      <td>8257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030272</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8258 rows × 3770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  h_10  h_2016    h_2017  h_america  h_attack  h_big  \\\n",
       "0              0   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "1              1   0.0     0.0  0.723634        0.0       0.0    0.0   \n",
       "2              2   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "3              3   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "4              4   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "...          ...   ...     ...       ...        ...       ...    ...   \n",
       "8253        8253   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "8254        8254   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "8255        8255   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "8256        8256   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "8257        8257   0.0     0.0  0.000000        0.0       0.0    0.0   \n",
       "\n",
       "      h_campaign  h_clinton  h_deal  ...  b_york city  b_york times  \\\n",
       "0            0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "1            0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "2            0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "3            0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "4            0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "...          ...        ...     ...  ...          ...           ...   \n",
       "8253         0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "8254         0.0        0.0     0.0  ...          0.0      0.025808   \n",
       "8255         0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "8256         0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "8257         0.0        0.0     0.0  ...          0.0      0.000000   \n",
       "\n",
       "      b_york times newsletter  b_york times product   b_young  b_young people  \\\n",
       "0                         0.0                   0.0  0.000000        0.000000   \n",
       "1                         0.0                   0.0  0.000000        0.000000   \n",
       "2                         0.0                   0.0  0.020271        0.000000   \n",
       "3                         0.0                   0.0  0.000000        0.000000   \n",
       "4                         0.0                   0.0  0.000000        0.000000   \n",
       "...                       ...                   ...       ...             ...   \n",
       "8253                      0.0                   0.0  0.000000        0.000000   \n",
       "8254                      0.0                   0.0  0.000000        0.000000   \n",
       "8255                      0.0                   0.0  0.036137        0.028606   \n",
       "8256                      0.0                   0.0  0.000000        0.000000   \n",
       "8257                      0.0                   0.0  0.000000        0.000000   \n",
       "\n",
       "       b_youth  b_youtube    b_zero  b_zone  \n",
       "0     0.000000   0.000000  0.000000     0.0  \n",
       "1     0.000000   0.000000  0.000000     0.0  \n",
       "2     0.031749   0.000000  0.000000     0.0  \n",
       "3     0.000000   0.000000  0.000000     0.0  \n",
       "4     0.000000   0.338232  0.000000     0.0  \n",
       "...        ...        ...       ...     ...  \n",
       "8253  0.000000   0.000000  0.000000     0.0  \n",
       "8254  0.000000   0.000000  0.000000     0.0  \n",
       "8255  0.000000   0.000000  0.000000     0.0  \n",
       "8256  0.000000   0.000000  0.000000     0.0  \n",
       "8257  0.000000   0.000000  0.030272     0.0  \n",
       "\n",
       "[8258 rows x 3770 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2932</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5489</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8253</th>\n",
       "      <td>2895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>7813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8255</th>\n",
       "      <td>905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>5192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8257</th>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8258 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Label\n",
       "0           2932      0\n",
       "1           1768      0\n",
       "2           5489      1\n",
       "3           4991      1\n",
       "4           5287      0\n",
       "...          ...    ...\n",
       "8253        2895      1\n",
       "8254        7813      0\n",
       "8255         905      1\n",
       "8256        5192      0\n",
       "8257         235      0\n",
       "\n",
       "[8258 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>h_10</th>\n",
       "      <th>h_2016</th>\n",
       "      <th>h_2017</th>\n",
       "      <th>h_america</th>\n",
       "      <th>h_attack</th>\n",
       "      <th>h_big</th>\n",
       "      <th>h_campaign</th>\n",
       "      <th>h_clinton</th>\n",
       "      <th>h_deal</th>\n",
       "      <th>...</th>\n",
       "      <th>b_york city</th>\n",
       "      <th>b_york times</th>\n",
       "      <th>b_york times newsletter</th>\n",
       "      <th>b_york times product</th>\n",
       "      <th>b_young</th>\n",
       "      <th>b_young people</th>\n",
       "      <th>b_youth</th>\n",
       "      <th>b_youtube</th>\n",
       "      <th>b_zero</th>\n",
       "      <th>b_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>2060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>2062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>2063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>2064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2065 rows × 3770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  h_10  h_2016  h_2017  h_america  h_attack  h_big  \\\n",
       "0              0   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "1              1   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "2              2   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "3              3   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "4              4   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "...          ...   ...     ...     ...        ...       ...    ...   \n",
       "2060        2060   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "2061        2061   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "2062        2062   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "2063        2063   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "2064        2064   0.0     0.0     0.0        0.0       0.0    0.0   \n",
       "\n",
       "      h_campaign  h_clinton  h_deal  ...  b_york city  b_york times  \\\n",
       "0            0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "1            0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "2            0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "3            0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "4            0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "...          ...        ...     ...  ...          ...           ...   \n",
       "2060         0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "2061         0.0   1.000000     0.0  ...          0.0           0.0   \n",
       "2062         0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "2063         0.0   0.561864     0.0  ...          0.0           0.0   \n",
       "2064         0.0   0.000000     0.0  ...          0.0           0.0   \n",
       "\n",
       "      b_york times newsletter  b_york times product   b_young  b_young people  \\\n",
       "0                         0.0                   0.0  0.000000             0.0   \n",
       "1                         0.0                   0.0  0.000000             0.0   \n",
       "2                         0.0                   0.0  0.000000             0.0   \n",
       "3                         0.0                   0.0  0.000000             0.0   \n",
       "4                         0.0                   0.0  0.000000             0.0   \n",
       "...                       ...                   ...       ...             ...   \n",
       "2060                      0.0                   0.0  0.000000             0.0   \n",
       "2061                      0.0                   0.0  0.000000             0.0   \n",
       "2062                      0.0                   0.0  0.000000             0.0   \n",
       "2063                      0.0                   0.0  0.174811             0.0   \n",
       "2064                      0.0                   0.0  0.000000             0.0   \n",
       "\n",
       "      b_youth  b_youtube  b_zero  b_zone  \n",
       "0         0.0        0.0     0.0     0.0  \n",
       "1         0.0        0.0     0.0     0.0  \n",
       "2         0.0        0.0     0.0     0.0  \n",
       "3         0.0        0.0     0.0     0.0  \n",
       "4         0.0        0.0     0.0     0.0  \n",
       "...       ...        ...     ...     ...  \n",
       "2060      0.0        0.0     0.0     0.0  \n",
       "2061      0.0        0.0     0.0     0.0  \n",
       "2062      0.0        0.0     0.0     0.0  \n",
       "2063      0.0        0.0     0.0     0.0  \n",
       "2064      0.0        0.0     0.0     0.0  \n",
       "\n",
       "[2065 rows x 3770 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4740</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>3821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>4668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>6153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>7233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>4395</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2065 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Label\n",
       "0           9145      0\n",
       "1           9773      1\n",
       "2           4740      1\n",
       "3           7867      1\n",
       "4            396      0\n",
       "...          ...    ...\n",
       "2060        3821      0\n",
       "2061        4668      0\n",
       "2062        6153      1\n",
       "2063        7233      1\n",
       "2064        4395      1\n",
       "\n",
       "[2065 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the loaded datasets to see if there are any issues\n",
    "display(X_train)\n",
    "display(y_train)\n",
    "display(X_test)\n",
    "display(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some added columns for each of the datasets. I will need to remove these before running models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix X_train\n",
    "X_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Fix y_train\n",
    "y_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Fix X_test\n",
    "X_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Fix y_train\n",
    "y_test.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will fit an unoptimised, out-of-the-box Logistic Regression Classifier to see what our base accuracy is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9509566480988133\n",
      "Test Accuracy: 0.9351089588377723\n"
     ]
    }
   ],
   "source": [
    "# fitting an out of the box logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "train_accuracy = logreg.score(X_train, y_train)\n",
    "test_accuracy = logreg.score(X_test,y_test)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training and test accuracy are fantastic, but also a little supsicious. I am aware that using NLP for Sentiment Analysis is not a simple task and was expecting an initial accuracy of around 60-70%. I suspect that there must be something trivial in the dataset that the model is picking up on. Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's coefficients\n",
    "coeffs = logreg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the labelling for the dataset is 0 for Fake News and 1 for Real News, this means that the large, positive coeffcients represent features that suggest a the article is Real. While large, negative coefficients are indicative of a feature that suggests the article is Fake. \n",
    "\n",
    "I will sort the coefficients and look at the top 10 and bottom 10 coefficients to see which features are most suggestive of the different types of news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3769,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the indices of the coefficients in ascending order by coefficient value (negative coefficients at the top)\n",
    "sorted_coeff_indices = np.argsort(coeffs)\n",
    "sorted_coeff_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  87,  743, 1613,  366, 2262, 3180, 1655, 2273, 2274, 3086])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 negative coefficients (Fake News features)\n",
    "fake_indices = sorted_coeff_indices[:10]\n",
    "fake_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2968,  455, 2576, 2157, 3555,  823,  596,  724, 2882, 2975])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 10 positive coefficients (Real News features)\n",
    "real_indices = sorted_coeff_indices[-10:]\n",
    "real_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['b_2016', 'b_com', 'b_hillary', 'b_article', 'b_not', 'b_source',\n",
       "       'b_http', 'b_november', 'b_november 2016', 'b_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the labels corresponding to the Fake News features\n",
    "X_train.columns[fake_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['b_sanders', 'b_bbc', 'b_president', 'b_monday', 'b_unfold',\n",
       "       'b_conservative', 'b_caption', 'b_cnn', 'b_reuters', 'b_say'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the labels corresponding to the Real News features\n",
    "X_train.columns[real_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the features that are most associated with Real and Fake News articles, there is one \"rule\" that the model appears to have picked up on. Notice that 3 of the features used to determine Real News are BBC, Reuters and CNN. These are reputable news sites and as such are likley to be sources of Real News and an article from that source will likely contain reference to themselves within the article's content. \n",
    "\n",
    "What this means is that the model has picked up on the fact that if the article comes from a reputable source it is likely to be Real News as opposed to Fake News. This is intuitively a good rule to follow, but is not what I am looking to achieve with my model.\n",
    "\n",
    "After some thinking I came up with a solution. Given that people on Facebook are often scrolling through a cluttered news feed they often won't read the article itself, but will only read the headline. In addition, the IPSOS study tested people's abilities to identify Fake News based solely on the article's headlines.\n",
    "\n",
    "As a result, I decided to drop the body from dataset and only use the article's headline to determine if the article is Fake News as it is more useful and will be less likely to use trivial features to make the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features from the article's body\n",
    "\n",
    "X_train_head = X_train\n",
    "X_test_head = X_test\n",
    "for col in X_train.columns:\n",
    "    if \"b_\" in col:\n",
    "        X_train_head.drop([col], axis=1, inplace=True)\n",
    "        X_test_head.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will re-run the out-of-the-box Logistic Regression model and see how this has affected the training and test accuracy. I will also rename the label datasets to match the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6277549043351901\n",
      "Test Accuracy: 0.6237288135593221\n"
     ]
    }
   ],
   "source": [
    "# Relabelling the target datasets\n",
    "y_train_head = y_train\n",
    "y_test_head = y_test\n",
    "\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "logreg.fit(X_train_head, y_train_head)\n",
    "\n",
    "# Training score\n",
    "train_accuracy = logreg.score(X_train_head, y_train_head)\n",
    "test_accuracy = logreg.score(X_test_head,y_test_head)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! These accuracies are more in line with what I was expecting. My goal from here would be to achieve an accuracy of over 75% using optimised models. I expect the Recurrent Neural Network will be the best model but we shall see.\n",
    "\n",
    "First I will save the new dataset with only the headline data, and then work on optimising my Logistic Regression model and see what parameters I can tune to improve the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and export the train and test Headline dataset\n",
    "X_train_head.to_csv('data/X_train_head.csv', header=True, index=False)\n",
    "X_test_head.to_csv('data/X_test_head.csv', header=True, index=False)\n",
    "\n",
    "y_train_head.to_csv('data/y_train_head.csv', header=True, index=False)\n",
    "y_test_head.to_csv('data/y_test_head.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising my model through Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of hyperparameters that can be optimised for Logistic Regression. The parameters that I will choose to optimise are: \\\n",
    "\n",
    "The penalty, either Lasso regression (L1), Ridge regression (l2). Ridge regression adds the square of the coefficients to the cost function will Lasso regression adds the magnitude of the coefficient. Lasso is often used as a form of feature selection as it will shrink less important coefficients to 0. I expect L1 to be the optimal penalty in this scenario.\n",
    "\n",
    "C, this controls the strength of the regularisation parameter as it is the inverse of the lambda regulator. The smaller C is, the stronger the regularisation will be.\n",
    "\n",
    "The Solver has a number of options for how to solve the optimisation problem of the cost function: newton-cg, lbfgs, liblinear, sag, saga.\n",
    "\n",
    "I was planning on using GridSearchCV to conduct this optimisation, however, a number of these parameter choices don't work together. So instead I will run a for loop and keep track of the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params: l1, 0.0001, liblinear\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.0001, saga\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.001, liblinear\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.001, saga\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.01, liblinear\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.01, saga\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 0.0001, 'liblinear']\n",
      "Best_test_score: 0.513317191283293\n",
      "Current params: l1, 0.1, liblinear\n",
      "Test score: 0.6188861985472155\n",
      "Best params: ['l1', 0.1, 'liblinear']\n",
      "Best_test_score: 0.6188861985472155\n",
      "Current params: l1, 0.1, saga\n",
      "Test score: 0.6188861985472155\n",
      "Best params: ['l1', 0.1, 'liblinear']\n",
      "Best_test_score: 0.6188861985472155\n",
      "Current params: l1, 1, liblinear\n",
      "Test score: 0.625181598062954\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 1, saga\n",
      "Test score: 0.625181598062954\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 10, liblinear\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 10, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 100, liblinear\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 100, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 1000, liblinear\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l1, 1000, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.0001, newton-cg\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.0001, lbfgs\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.0001, sag\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.0001, saga\n",
      "Test score: 0.513317191283293\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.001, newton-cg\n",
      "Test score: 0.5530266343825666\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.001, lbfgs\n",
      "Test score: 0.5530266343825666\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.001, sag\n",
      "Test score: 0.5530266343825666\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.001, saga\n",
      "Test score: 0.5530266343825666\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.01, newton-cg\n",
      "Test score: 0.6203389830508474\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.01, lbfgs\n",
      "Test score: 0.6203389830508474\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.01, sag\n",
      "Test score: 0.6203389830508474\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.01, saga\n",
      "Test score: 0.6203389830508474\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.1, newton-cg\n",
      "Test score: 0.6208232445520581\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.1, lbfgs\n",
      "Test score: 0.6208232445520581\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.1, sag\n",
      "Test score: 0.6208232445520581\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 0.1, saga\n",
      "Test score: 0.6208232445520581\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1, newton-cg\n",
      "Test score: 0.6237288135593221\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1, lbfgs\n",
      "Test score: 0.6237288135593221\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1, sag\n",
      "Test score: 0.6237288135593221\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1, saga\n",
      "Test score: 0.6237288135593221\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 10, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 10, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 10, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 10, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 100, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 100, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 100, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 100, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1000, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1000, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1000, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: l2, 1000, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.0001, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.0001, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.0001, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.0001, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.001, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.001, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.001, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.001, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.01, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.01, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.01, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.01, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.1, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.1, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params: none, 0.1, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 0.1, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 10, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 10, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 10, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 10, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 100, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 100, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 100, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 100, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1000, newton-cg\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1000, lbfgs\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1000, sag\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n",
      "Current params: none, 1000, saga\n",
      "Test score: 0.6232445520581114\n",
      "Best params: ['l1', 1, 'liblinear']\n",
      "Best_test_score: 0.625181598062954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "penalties = ['l1', 'l2', 'none']\n",
    "cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "best_params = []\n",
    "best_test_score = 0\n",
    "\n",
    "for penalty in penalties:\n",
    "    for c in cs:\n",
    "        for solver in solvers:\n",
    "            if (penalty == 'l1') and ((solver == 'newton-cg') or (solver == 'lbfgs') or (solver == 'sag')):\n",
    "                continue \n",
    "            elif (penalty == 'l2' or penalty == 'none') and (solver == 'liblinear'):\n",
    "                continue\n",
    "            else:\n",
    "                model = LogisticRegression(penalty=penalty, C=c, solver=solver, random_state=1)\n",
    "                model_fit = model.fit(X_train_head, y_train_head)\n",
    "                \n",
    "                test_score = model.score(X_test_head, y_test_head)\n",
    "                if test_score > best_test_score:\n",
    "                    best_params = [penalty, c, solver]\n",
    "                    best_test_score = test_score                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: ['l1', 1, 'liblinear']\n"
     ]
    }
   ],
   "source": [
    "# What are the best parameters?\n",
    "print(f'Best parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6281181884233471\n",
      "Test Accuracy: 0.625181598062954\n"
     ]
    }
   ],
   "source": [
    "# Re-run the model with the optimised parameters and see how much the training and test accuracies improve\n",
    "\n",
    "log_model = LogisticRegression(penalty=best_params[0], C=best_params[1], solver=best_params[2], random_state=1)\n",
    "log_model_fit = log_model.fit(X_train_head, y_train_head)\n",
    "\n",
    "# Training score\n",
    "train_accuracy = log_model.score(X_train_head, y_train_head)\n",
    "test_accuracy = log_model.score(X_test_head,y_test_head)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of that, we only got a .25% increase in our test_accuracy. This is not too surprising. Perhaps reducing the number of dimensions of my data using PCA will help improve my model. One issue with this is that it will become difficult to interpret how the model is making its classification decisions as I won't be able to extract specific words and their coefficients anymore. \n",
    "\n",
    "Let's give PCA a try and see if there is any improvement and if it would be worth sacrificing the explicative qualities of the model for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prinicpal Component Analysis for Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8258, 41)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_head.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current training and test sets have 41 features, which is not a lot compared to my original dataset. Even so, by reducing the number of dimensions we may be able to explain a large portion of the data's variance with a signficantly smaller number of features and improve the accuracy of our models. Here I will aim to find the best number of dimensions that explain 80% of the data's variance. Then I will transform my original dataset using these components and then re-optimise my Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit a PCA model to the data\n",
    "\n",
    "# 1. Instantiate with 41 components to have enough to see the elbo plot\n",
    "my_pca = PCA(n_components=41)\n",
    "\n",
    "# 2. Fit (mathematical calculations are made at this step) \n",
    "my_pca.fit(X_train_head)\n",
    "\n",
    "# 3. Transform\n",
    "X_PCA = my_pca.transform(X_train_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10994775 0.17161755 0.2146728  0.25440976 0.29160447 0.32363431\n",
      " 0.35435065 0.38448352 0.41425486 0.44303842 0.47095768 0.49795379\n",
      " 0.52284432 0.54688423 0.57048376 0.59366479 0.61636214 0.63898508\n",
      " 0.66131805 0.68329753 0.70504708 0.7265041  0.74649275 0.76613258\n",
      " 0.7852417  0.80365461 0.82188933 0.83912074 0.85612351 0.8725863\n",
      " 0.88808551 0.903048   0.91790746 0.9320736  0.94540515 0.95865196\n",
      " 0.9696751  0.98042361 0.9902783  0.99927237 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cumulative variance explained over the components\n",
    "expl_var_cumulative = my_pca.explained_variance_ratio_.cumsum()\n",
    "print(expl_var_cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the number of components that explains 80% of the variance \n",
    "plt.figure()\n",
    "plt.plot(range(1,42),expl_var_cumulative,marker='.')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1,42,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10994775 0.0616698  0.04305525 0.03973695 0.03719471 0.03202984\n",
      " 0.03071634 0.03013288 0.02977134 0.02878356 0.02791925 0.02699612\n",
      " 0.02489053 0.02403991 0.02359952 0.02318104 0.02269735 0.02262294\n",
      " 0.02233296 0.02197949 0.02174955 0.02145702 0.01998864 0.01963983\n",
      " 0.01910912 0.01841291 0.01823472 0.0172314  0.01700277 0.0164628\n",
      " 0.01549921 0.01496249 0.01485946 0.01416613 0.01333156 0.01324681\n",
      " 0.01102314 0.01074851 0.00985469 0.00899406 0.00072763]\n"
     ]
    }
   ],
   "source": [
    "# Get the explained variance ratio for each component\n",
    "expl_var = my_pca.explained_variance_ratio_\n",
    "print(expl_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the elbow in the scree plot\n",
    "plt.figure()\n",
    "plt.plot(range(1,42),expl_var,marker='.')\n",
    "plt.xlabel('Number of PCs')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1,42,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two plots above, we have a decision to make. Based on the scree plot, the elbow appears to occur around 3 components. When we look at the cumulative variance explained, however, 3 components only explains 20% of the variance. I expect this is too low and will siginificantly reduce the acccuracy of my model as I will be discarding a significant amount of information. It looks as though 25 components will explain 80% of the variance, however, to compare and see if there is an optimal number of components I will run a for loop over a range from 1 to 31, as I want to have some form of dimensionality reduction, and see which is the best outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_range = np.arange(1, 32, 5)\n",
    "best_component = 0\n",
    "best_PCA_test_score = 0\n",
    "best_PCAPparams = []\n",
    "best_test_score = 0\n",
    "\n",
    "for component in components_range:\n",
    "\n",
    "    # Train on the components\n",
    "    my_pca = PCA(n_components=component)\n",
    "    my_pca.fit(X_train_head)\n",
    "\n",
    "    # transform data onto the components\n",
    "    X_train_PCA = my_pca.transform(X_train_head)\n",
    "\n",
    "    # Transform test data\n",
    "    X_test_PCA = my_pca.transform(X_test_head)\n",
    "\n",
    "\n",
    "\n",
    "    # Re-optimise the Logistic Regression with the transformed dataset\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    penalties = ['l1', 'l2', 'none']\n",
    "    cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "    best_params = []\n",
    "    best_test_score = 0\n",
    "    \n",
    "    for penalty in penalties:\n",
    "        for c in cs:\n",
    "            for solver in solvers:\n",
    "                if (penalty == 'l1') and ((solver == 'newton-cg') or (solver == 'lbfgs') or (solver == 'sag')):\n",
    "                    continue \n",
    "                elif (penalty == 'l2' or penalty == 'none') and (solver == 'liblinear'):\n",
    "                    continue\n",
    "                else:\n",
    "                    model = LogisticRegression(penalty=penalty, C=c, solver=solver, random_state=1)\n",
    "                    model_fit = model.fit(X_train_PCA, y_train_head)\n",
    "\n",
    "                    test_score = model.score(X_test_PCA, y_test_head)\n",
    "                    if test_score > best_test_score:\n",
    "                        best_params = [penalty, c, solver]\n",
    "                        best_test_score = test_score\n",
    "                    \n",
    "    if best_test_score > best_PCA_test_score:\n",
    "        best_PCA_test_score = best_test_score\n",
    "        best_PCA_params = best_params\n",
    "        best_component = component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: ['l1', 0.1, 'saga']\n",
      "Best parameters: 26\n"
     ]
    }
   ],
   "source": [
    "# What are the best parameters?\n",
    "print(f'Best parameters: {best_PCA_params}')\n",
    "\n",
    "# What is the best number of components?\n",
    "print(f'Best parameters: {best_component}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6144344877694357\n",
      "Test Accuracy: 0.6188861985472155\n"
     ]
    }
   ],
   "source": [
    "# Re-run the model with the optimised parameters with the data transformed using the optimal number of components \n",
    "# see how much the training and test accuracies improve\n",
    "\n",
    "# Train on the components\n",
    "my_pca = PCA(n_components=best_component)\n",
    "my_pca.fit(X_train_head)\n",
    "\n",
    "# transform data onto the components\n",
    "X_train_PCA = my_pca.transform(X_train_head)\n",
    "\n",
    "# Transform test data\n",
    "X_test_PCA = my_pca.transform(X_test_head)\n",
    "\n",
    "log_model = LogisticRegression(penalty=best_PCA_params[0], C=best_PCA_params[1], \n",
    "                               solver=best_PCA_params[2], random_state=1)\n",
    "log_model_fit = log_model.fit(X_train_PCA, y_train_head)\n",
    "\n",
    "# Training score\n",
    "train_accuracy = log_model.score(X_train_PCA, y_train_head)\n",
    "test_accuracy = log_model.score(X_test_PCA, y_test_head)\n",
    "\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our model did decrease when reducing the number of dimensions. This is not too surprising as we are removing information about our data when we reduce the number of components. That being said we lost less than 0.7% accuracy but reduced our dataset from 41 to 26 features. This would be seen as a reasonable sacrifice for an improvement in performance,  but hopefully we can improve our detector's accuracy using a different model. \n",
    "\n",
    "Up next, Random Forests!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
